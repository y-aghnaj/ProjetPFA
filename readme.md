# Cloud Governance Audit Prototype (OCI)

This project is a functional prototype of an AI-assisted Cloud Governance Audit system applied to Oracle Cloud Infrastructure (OCI). Its purpose is to demonstrate how cloud governance concepts such as security, performance, explainability, and the Shared Responsibility Model (SRM) can be integrated into a single, coherent, and auditable architecture. The prototype is intentionally explainable and deterministic: all analysis is rule-based and traceable, while artificial intelligence is used only at the final stage to generate a human-readable audit report.

The system follows a clear and structured pipeline. A mock OCI environment is first described as a JSON state. This state is then transformed into a resource graph, where cloud resources such as Object Storage buckets, databases, network security groups, compute instances, and users are represented as nodes, and their relationships are represented as edges. On top of this graph, a rule engine evaluates governance conditions and produces findings. These findings are then aggregated using a risk-based scoring model, followed by the generation of actionable recommendations and, finally, an automatically generated audit report using a local language model.

The choice of a graph-based model is motivated by the relational nature of cloud environments. Resources are never isolated; their security and governance posture depends on how they interact. Modeling the environment as a graph preserves context, traceability, and extensibility, which are essential properties for audit and governance use cases. The rule engine is deterministic by design. Each rule corresponds to a well-defined governance or security issue, such as public storage exposure, missing encryption, insecure network configuration, or performance inefficiencies. This approach was chosen because explainability and auditability are critical in governance contexts, often more important than prediction accuracy.

Each finding generated by the system contains a rule identifier, a severity level, a responsibility attribution according to the Shared Responsibility Model, and a detailed evidence object. The evidence explicitly lists the configuration parameters that triggered the rule, ensuring that every result can be justified to an auditor. Findings are classified by governance pillar, mainly Security and Performance, reflecting real-world frameworks such as the Well-Architected Framework.

Scoring is performed using a risk-based aggregation model rather than a simple penalty summation. Each finding contributes a normalized risk value depending on its severity, and risks are aggregated using a probabilistic formula with saturation: Risk = 1 − Π(1 − r_i). This approach avoids naive accumulation, better represents how risks compound in real systems, and is commonly used in risk management disciplines. From the aggregated risks, the system computes a Security Score, a Performance Score, and a weighted Global Score.

Recommendations are generated only after findings and scoring have been computed. This ensures that recommendations are traceable, justified, and directly linked to identified issues. Each recommendation includes concrete remediation steps, a rationale, and a clear responsibility attribution. The Shared Responsibility Model is central to the design: in this prototype, most findings fall under customer responsibility, while the cloud provider’s responsibilities remain at the baseline infrastructure level and are not directly remediated by the customer.

Artificial intelligence is deliberately limited to report generation. A local language model, executed via Ollama, consumes the structured JSON output produced by the system and generates a professional, audit-style Markdown report. The model is constrained by a strict prompt to avoid hallucinations or invented data. Using a local LLM ensures data confidentiality, reproducibility, and full control over the generation process.

The final output of the prototype includes a resource graph, a list of explainable findings, justified scores, actionable recommendations, and an automatically generated audit report. The prototype has intentional limitations: it uses mock data rather than real OCI APIs, rules are static, there is no automatic remediation, and the language model is not used for analysis or decision-making. These limitations are deliberate and serve to preserve explainability, auditability, and pedagogical clarity.

The project structure is organized as follows: a data directory containing the mock OCI environment, modules for graph construction, rule definition, scoring, and recommendation generation, a reporting module for LLM-based report generation, a reports directory containing the structured JSON output and the generated Markdown report, and a main entry point that orchestrates the full pipeline.

This prototype is designed as an academic and demonstrative project focused on cloud governance, security engineering, explainable AI, and risk-based assessment. The architecture is extensible and could be enhanced in future work with real OCI API integration, additional governance frameworks such as ISO 27001 or NIST, advanced graph analytics, or PDF report export.
