# Cloud Governance Audit Framework

Cloud Governance Audit Framework is a deterministic, graph-based cloud auditing engine designed to assess infrastructure configurations against governance, security, performance, and cost principles aligned with the Well-Architected Framework (WAF). The system is built around modular components that separate infrastructure modeling, rule evaluation, scoring logic, and reporting, ensuring extensibility, traceability, and architectural clarity.

The framework operates by ingesting a structured infrastructure snapshot (OCI, AWS, or Azure), normalizing provider-specific metadata into a canonical internal representation, and constructing a directed resource graph using NetworkX. This graph representation enables both atomic rule evaluation and graph-based reasoning, such as attack path detection or exposure chaining. A deterministic rule engine then evaluates the graph against registered security, performance, and composite rules. Composite rules provide risk aggregation and suppression logic to prevent redundant findings and double counting.

Scoring is fully aligned with the Well-Architected Framework pillars. Each finding maps to one or multiple pillars, and severity levels translate into deterministic penalty values. When a finding affects multiple pillars, the penalty is proportionally split to ensure scoring consistency. The global score is computed as a weighted aggregation of individual pillar scores, guaranteeing reproducibility and mathematical transparency.

Pillar weights are managed exclusively by the `WeightCalculator` component located in `governance/weight_calculator.py`. This design removes arbitrariness from the scoring engine and enforces a single source of truth for weight management. Default weights are currently defined as SECURITY (0.30), RELIABILITY (0.20), PERFORMANCE (0.15), COST (0.15), and OPERATIONAL_EXCELLENCE (0.20). The architecture is intentionally designed to support future dynamic weighting algorithms based on contextual risk, organizational posture, or statistical inference without modifying the scoring core.

The framework supports both static and AI-assisted recommendation generation. Static recommendations are derived deterministically from findings, while optional LLM-based recommendations and Markdown reporting can be generated using Ollama. AI components are intentionally decoupled from core scoring logic to preserve determinism and audit traceability.

Audit results are exported as structured JSON documents containing summary metadata, findings, pillar scores, global score, applied weights, recommendations, control catalog statistics, and a DOT-format graph representation of the infrastructure. Baseline comparison functionality allows differential analysis between infrastructure states, enabling governance drift detection and score evolution tracking.

The project structure is organized into clearly separated modules: `app` for orchestration and diffing, `graph` for infrastructure modeling, `rules` for security and performance logic, `governance` for WAF definitions and weight management, `scoring` for deterministic score computation, `recommendations` for remediation logic, `reporting` for LLM report generation, `providers` for cloud normalization adapters, and `data` for scenario snapshots.

This framework is intended for advanced cloud security engineering, governance research, and deterministic risk modeling. It emphasizes explainability, modularity, and mathematical traceability over heuristic or opaque AI-driven scoring approaches, making it suitable for academic research, enterprise governance prototyping, and security architecture experimentation.
